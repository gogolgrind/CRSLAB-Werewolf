{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Werewolf Game Video Analysis Project\n",
    "#### Experimental design and implementation by K. Sozykin\n",
    "#### Dataset, idea by Nurmukhamet Abdullin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt\n",
    "from scipy.signal import butter,filtfilt,firwin,medfilt,lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense,Dropout, Activation,LSTM,Flatten,Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from keras_squeezenet import SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score , f1_score,jaccard_similarity_score,confusion_matrix\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.utils import shuffle as sk_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from  xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global wsize\n",
    "wsize = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vids_dir = os.path.join('..','data','video')\n",
    "auds_dir = os.path.join('..','data','audio')\n",
    "label_path = os.path.join('..','data','ground_truth2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert os.path.exists(vids_dir) and os.path.exists(auds_dir) and os.path.exists(label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vid2data_cv2(vname,vid_path,interv = None,grayscale = False,res_c = 1,starts = []):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(vid_path)\n",
    "    except:\n",
    "        print (\"problem opening input stream\")\n",
    "        return\n",
    "    if not cap.isOpened():\n",
    "        print (\"capture stream not open\")\n",
    "        return\n",
    "    prev = 0\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps    = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    global wsize\n",
    "    wsize = int(fps)\n",
    "    print(\"%s has fps %s len %s\" %(vname,fps,length))\n",
    "    dataset = {\n",
    "        'vid_features' : [[] for e in range(13)],\n",
    "        'motion_series' : [[] for e in range(13)],\n",
    "        'labels' : [],\n",
    "        vname : [[] for e in range(13)],\n",
    "    }\n",
    "    starts = fps * array(starts)\n",
    "    step = 1\n",
    "    ok_idx = ones([length,]).astype(bool)\n",
    "    sample_windows = [0] +  np.random.permutation(arange(1,11))[:3].tolist()\n",
    "    print('sample windows',sample_windows)\n",
    "    ok_idx[:starts[0]] = False \n",
    "    corr_idx = array([1,2,3,4,5,6,9,10,7,8])-1\n",
    "    for fr_idx in tqdm(range(length)):\n",
    "        \n",
    "        ret, fr = cap.read()\n",
    "        \n",
    "        if (fr_idx > 0 and fr_idx % fps == 0):\n",
    "\n",
    "            \n",
    "            q1 = qsplit(fr)\n",
    "            q2 = [qsplit(e) for e in q1[1:]]\n",
    "            player_shape = q2[0][0].shape\n",
    "            main_window = [q1[0]]\n",
    "            players_windows = [e for e in array(q2).reshape([12] + list(player_shape))]\n",
    "            players_windows = [players_windows[idx] for idx in corr_idx]\n",
    "            \n",
    "            windows = main_window + players_windows\n",
    "            \n",
    "            diff = np.abs(array(prev)-array(players_windows))\n",
    "            for j,e in enumerate(dataset['motion_series']):\n",
    "                dataset['motion_series'][j].append(sum(sum(sum(diff))))\n",
    "            prev = players_windows\n",
    "            # vgg 224x224\n",
    "            target_size = (227, 227)\n",
    "            windows = array([ cv2.resize(e,target_size).astype('float32') for e in windows ])\n",
    "            \n",
    "            #windows = windows[sample_windows]\n",
    "            features  = imnet_features(windows)\n",
    "            for idx1,idx2 in enumerate(sample_windows):\n",
    "                dataset['vid_features'][idx1].append(features[idx2])\n",
    "            for idx,f in enumerate(features):\n",
    "                dataset[vname][idx].append(f) \n",
    "            lb = 0\n",
    "            cur_sec = int(round(fr_idx/fps))\n",
    "            if interv != None:\n",
    "                for k in range(0,len(interv),2):\n",
    "                    if cur_sec >= interv[k] and cur_sec <= interv[k + 1]:\n",
    "                        lb = 1\n",
    "                        break \n",
    "\n",
    "            dataset['labels'].append(lb)\n",
    "    cap.release()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vid2data = vid2data_skv if os.name == 'posix' else vid2data_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowpass_filter(x,order = 10,Fc = 40, Fs = 1600):\n",
    "    # provide them to firwin\n",
    "    h = firwin(numtaps=order, cutoff=Fc, nyq=Fs/2)\n",
    "    # 'x' is the time-series data you are filtering\n",
    "    y = lfilter(h, 1.0, x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nn_model_video(opt = Adam(),nb_lstm1 = 256,p = 0.5,data_dim = 1000,timesteps = wsize,nb_classes=2):\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(nb_lstm1, return_sequences=True,activation = 'linear'),input_shape = [timesteps,data_dim]))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer = opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model_audio(opt = Adam(),nb_lstm1 = 256,p = 0.5,data_dim = 1000,timesteps = wsize,nb_classes=2):\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(nb_lstm1, return_sequences=True,activation = 'sigmoid'),input_shape = [timesteps,data_dim]))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(Bidirectional(LSTM(nb_lstm1//2, return_sequences=True,activation = 'sigmoid')))\n",
    "    model.add(Dropout(p)),\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer = opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imnet = VGG19(weights='imagenet')\n",
    "#imnet_extractor = Model(input=imnet.input, output=imnet.get_layer('fc1').output)\n",
    "imnet = SqueezeNet()\n",
    "imnet_extractor = Model(inputs=imnet.input, outputs=imnet.get_layer('loss').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imnet_features(x):\n",
    "    \"\"\"\n",
    "        return vgg fc1 features for particlurar image \n",
    "    \"\"\"\n",
    "    mean = [103.939,116.779,123.68]\n",
    "    x = array([ (e-mean) for e in x])\n",
    "    return imnet_extractor.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interv_cols = \"Day0 Night1 Day1 Night2 Day2 Night3 Day3 Night4 Day4 Night5\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtruth = xls.parse('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vlist = file_list(vids_dir,'Cap02*.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vlist = np.random.permutation(vlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['vlist'] = vlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction From Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx,v in enumerate(vlist[:]):\n",
    "    sep = '/' if os.name == 'posix' else '\\\\' \n",
    "    vname = v.split(sep)[-1]\n",
    "    match_sz = len(gtruth[gtruth.File == vname][interv_cols].values)\n",
    "    #dataset['vlist'].append(vname)\n",
    "    starts =  []\n",
    "    ends =  []\n",
    "    interv = []\n",
    "    for i in range(match_sz):\n",
    "        interv += [t2sec(e)  for e in gtruth[gtruth.File == vname][interv_cols].values[i] if str(e)!='nan']\n",
    "    \n",
    "    starts += [t2sec(e)  for e in gtruth[gtruth.File == vname]['Start'].values if str(e)!='nan']\n",
    "    ends += [t2sec(e)  for e in gtruth[gtruth.File == vname]['End'].values if str(e)!='nan']\n",
    "    print(v,interv)\n",
    "    ds = vid2data(vname,v,interv = interv,starts = starts)\n",
    "    if idx == 0:\n",
    "        dataset = ds\n",
    "    else:\n",
    "        dataset['labels'] += ds['labels']\n",
    "        dataset[vname] = ds[vname]\n",
    "        for i in range(len(dataset['vid_features'])):\n",
    "            dataset['motion_series'][i] += ds['motion_series'][i]\n",
    "            dataset['vid_features'][i] += ds['vid_features'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction From Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alist = array([e.replace('video','audio').replace('mp4','opus') for e in vlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset['audio_features'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_audio(fname):\n",
    "    r = []\n",
    "    x,fs = librosa.load(fname)\n",
    "    sec_step = x.shape[0]//fs\n",
    "    for t in range(0,sec_step):\n",
    "        sec = x[t*fs+1:(t+1)*fs]\n",
    "        r.append(sec)\n",
    "    return (array(r),fs)\n",
    "\n",
    "def get_mfcc(spec,fs):\n",
    "    mfccf = array([librosa.feature.mfcc(e,fs,n_mfcc = 40) for e in spec])\n",
    "    mfccf = array([e.flatten() for e in mfccf])\n",
    "    mfccf = (mfccf - mfccf.mean(0))/mfccf.std(0)\n",
    "    return mfccf\n",
    "\n",
    "def get_chroma_stft(spec,fs):\n",
    "    chroma_stft = array([librosa.feature.chroma_stft(e,fs) for e in spec])\n",
    "    chroma_stft = array([e.flatten() for e in chroma_stft])\n",
    "    chroma_stft = (chroma_stft - chroma_stft.mean(0))/chroma_stft.std(0)\n",
    "    return chroma_stft\n",
    "\n",
    "def get_spectral_center(spec,fs):\n",
    "    spectral_center = array([librosa.feature.spectral_centroid(e,fs) for e in spec])\n",
    "    spectral_center = array([e.flatten() for e in spectral_center])\n",
    "    spectral_center = (spectral_center - spectral_center.mean(0))/spectral_center.std(0)\n",
    "    return spectral_center\n",
    "\n",
    "def get_spectral_rolloff(spec,fs):\n",
    "    spectral_rolloff = array([librosa.feature.spectral_rolloff(e,fs) for e in spec])\n",
    "    spectral_rolloff = array([e.ravel() for e in spectral_rolloff])\n",
    "    spectral_rolloff = (spectral_rolloff - spectral_rolloff.mean(0))/spectral_rolloff.std(0)\n",
    "    return spectral_rolloff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in alist[:]:\n",
    "    \n",
    "    aname = e.split('\\\\')[-1]\n",
    "    print(e,aname)\n",
    "    wave,fs = split_audio(e)\n",
    "    mfccf = get_mfcc(wave,fs)\n",
    "    spectral_center = get_spectral_center(wave,fs)\n",
    "    chroma_stft = get_chroma_stft(wave,fs)\n",
    "    spectral_rolloff = get_spectral_rolloff(wave,fs)\n",
    "    t = hstack([mfccf,spectral_center,spectral_rolloff])\n",
    "    dataset[aname] =  t\n",
    "    dataset['audio_features'] += t.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if do_load:\n",
    "    fname = 'dataset_2017-04-25__02-41-36.pkl'\n",
    "    with open(fname,'rb') as f:\n",
    "        dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def day_night_experiment(experiment_type = 2,verbose = 0):\n",
    "    U,V = [],[]\n",
    "    # number of player frames\n",
    "    N = 3\n",
    "    if experiment_type == 1:\n",
    "        nb_epoch=100\n",
    "        avg_motion = np.mean(dataset['motion_series'],0)\n",
    "        avg_motion = lowpass_filter(avg_motion)\n",
    "        avg_motion = (avg_motion -  avg_motion.mean())/avg_motion.std()\n",
    "        motion_lbs = array(dataset['labels'])\n",
    "        motion_lbs = motion_lbs[avg_motion > 0]\n",
    "        avg_motion = avg_motion[avg_motion > 0]\n",
    "        assert len(avg_motion) == len(motion_lbs)\n",
    "        \n",
    "        X = avg_motion\n",
    "        y = motion_lbs\n",
    "        w = get_class_weight({0 : len(y[y==0]), 1 : len(y[y==1])})\n",
    "    elif experiment_type == 2:\n",
    "        nb_epoch=100\n",
    "        y = dataset['labels']\n",
    "        for v in sliding_window(y,wsize,wsize//5):\n",
    "            V.append([[1,0] if e == 0 else [0,1] for e in v])\n",
    "        U_players = [[] for i in range(N)] \n",
    "        \n",
    "        for i in arange(1,N+1): \n",
    "            X =  array(dataset['vid_features'][i])\n",
    "            \n",
    "            for u in zip(sliding_window(X,wsize,wsize//5)):\n",
    "                U_players[i-1].append(u)\n",
    "            print(array(U_players[i-1]).shape,len(V))\n",
    "        U = array([U_players[np.random.randint(0,3)][k][0] for k in range(len(V))])\n",
    "        V = array(V)\n",
    "    elif experiment_type == 3:\n",
    "        nb_epoch=100\n",
    "        X,y = array(dataset['vid_features'][0]),dataset['labels']\n",
    "    elif experiment_type == 4:\n",
    "        nb_epoch=100\n",
    "        X,y = array(dataset['audio_features']),dataset['labels']\n",
    "    if experiment_type != 2:\n",
    "        for u,v in zip(sliding_window(X,wsize,wsize//5),\n",
    "                        sliding_window(y,wsize,wsize//5)):\n",
    "            U.append(u)\n",
    "            V.append([[1,0] if e == 0 else [0,1] for e in v])\n",
    "        U,V = array(U),array(V)\n",
    "        if len(U.shape) == 2:\n",
    "            U  = U[...,np.newaxis]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    print(\"Datset shapes:\",U.shape,V.shape)\n",
    "    if experiment_type != 4:\n",
    "        nn = nn_model_video(data_dim=U.shape[-1],timesteps = U.shape[-2])\n",
    "    else:\n",
    "        nn = nn_model_audio(data_dim=U.shape[-1],timesteps = U.shape[-2])\n",
    "    U_train, U_test, v_train, v_test = train_test_split(U, V, test_size=0.3, random_state=7)\n",
    "    \n",
    "    nn.fit(U_train,v_train,epochs=nb_epoch,batch_size=128,verbose=verbose)\n",
    "    v_pred_proba = nn.predict_proba(U_test)\n",
    "    v_test_hat = []\n",
    "    v_pred  = []\n",
    "    for seq in v_test:\n",
    "        v_test_hat.append(np.argmax(seq,1))\n",
    "    v_test_hat = array(v_test_hat)\n",
    "    for seq in v_pred_proba:\n",
    "         v_pred.append(np.argmax(seq,1))\n",
    "    v_pred = array(v_pred)\n",
    "    scores = {\n",
    "        'jaccard': [], \n",
    "        'hamming_loss' : [],\n",
    "        'accuracy' : []\n",
    "    }\n",
    "    night_samples = v_test_hat.sum(1)!=0\n",
    "    for t,f,s in zip(v_test_hat,v_pred,v_pred_proba):\n",
    "        j = jaccard_similarity_score(t,f)\n",
    "        h = hamming_loss(t,f)\n",
    "        acc = accuracy_score(t,f)\n",
    "        scores['jaccard'].append(j)\n",
    "        scores['hamming_loss'].append(h)\n",
    "    \n",
    "    scores['jaccard'] = array(scores['jaccard'])\n",
    "    scores['hamming_loss'] = array(scores['hamming_loss'])\n",
    "    #alpha = array([sum(f)/sum(t) if sum(t) > 0  and sum(f) > 0 else 0  for t,f in zip(v_test_hat,v_pred) ])\n",
    "    #alpha = alpha[alpha>0]\n",
    "    f1 = f1_score(v_test_hat.ravel(),v_pred.ravel())\n",
    "    f1_night = f1_score(v_test_hat[night_samples].ravel(),v_pred[night_samples].ravel())\n",
    "    qtable  = array([\n",
    "        [np.mean(scores['hamming_loss']),np.mean(scores['jaccard']),f1],\n",
    "        [np.mean(scores['hamming_loss'][night_samples]),np.mean(scores['jaccard'][night_samples]),f1_night]\n",
    "    ])\n",
    "    cols = ['hamming_loss','jaccard','f1*']\n",
    "    qtable = pd.DataFrame(qtable,columns = cols, index = ['total','night'])\n",
    "    \n",
    "    return (nn,qtable,v_test_hat,v_pred,v_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn1,df1,v_test1,v_pred1,v_probs1 = day_night_experiment(1,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn2,df2,v_test2,v_pred2,v_probs2 = day_night_experiment(2,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn3,df3,v_test3,v_pred3,v_probs3 = day_night_experiment(3,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn4,df4,v_test4,v_pred4,v_probs4 =  day_night_experiment(4,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_df = pd.concat([df1,df2,df3,df4], axis=0,names  = ['1','2','3','4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Combos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR RULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.all(v_test3 == v_test4) and np.all(v_test2 == v_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_pred234 = np.logical_or(v_pred2,v_pred3,v_pred4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "night_samples = v_test3.sum(1)!=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_combo = f1_score(v_pred234.ravel(),v_test3.ravel())\n",
    "f1_combo_night = f1_score(v_pred234[night_samples].ravel(),v_test3[night_samples].ravel())\n",
    "jaccard_combo,hamming_loss_combo = [],[]\n",
    "\n",
    "for t,f in zip(v_pred234,v_test3):\n",
    "    jaccard_combo.append(jaccard_similarity_score(t,f))\n",
    "    hamming_loss_combo.append(hamming_loss(t,f))\n",
    "jaccard_combo_mean,hamming_loss_mean = np.mean(jaccard_combo),np.mean(hamming_loss_combo)\n",
    "jaccard_combo_mean_night = np.mean(array(jaccard_combo)[night_samples])\n",
    "hamming_loss_mean_night = np.mean(array(hamming_loss_combo)[night_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo_or = array([[hamming_loss_mean,jaccard_combo_mean,f1_combo],\n",
    "             [hamming_loss_mean_night,jaccard_combo_mean_night,f1_combo_night]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo_or = pd.DataFrame(combo_or,\n",
    "             columns='hamming_loss,jaccard,f1*'.split(','),index = 'total,night'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average  probs rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_proba,v_pred_avg = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_proba = array([((p2+p3+p4)/3) for p2,p3,p4 in zip(v_probs2,v_probs3,v_probs4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_pred_avg = array([np.argmax(seq,1) for seq in avg_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1_avg = f1_score(v_pred_avg.ravel(),v_test3.ravel())\n",
    "f1_avg_night = f1_score(v_pred_avg[night_samples].ravel(),v_test3[night_samples].ravel())\n",
    "jaccard_avg,hamming_loss_avg = [],[]\n",
    "\n",
    "for t,f in zip(v_pred_avg,v_test3):\n",
    "    jaccard_avg.append(jaccard_similarity_score(t,f))\n",
    "    hamming_loss_avg.append(hamming_loss(t,f))\n",
    "jaccard_avg_mean,hamming_loss_mean = np.mean(jaccard_avg),np.mean(hamming_loss_avg)\n",
    "jaccard_avg_mean_night = np.mean(array(jaccard_avg)[night_samples])\n",
    "hamming_loss_mean_night = np.mean(array(hamming_loss_avg)[night_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combo_avg = array([[hamming_loss_mean,jaccard_avg_mean,f1_combo],\n",
    "             [hamming_loss_mean_night,jaccard_avg_mean_night,f1_avg_night]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combo_avg = pd.DataFrame(combo_avg,\n",
    "             columns='hamming_loss,jaccard,f1*'.split(','),index = 'total,night'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df = pd.concat([total_df,combo_or,combo_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving  data and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "total_df.to_csv('total_df_combo_all_'+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if do_save:\n",
    "    now = datetime.datetime.now().strftime(\"%Y-%m-%d__%H-%M-%S\")\n",
    "    fname = 'dataset_'+now + '.pkl'\n",
    "    print(fname)\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Werewolf detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_wlf_dataset(dstype = 1):\n",
    "    y = []\n",
    "    X = []\n",
    "    for e in vlist[:]:\n",
    "        vname = e.split('\\\\')[-1]\n",
    "        aname = vname.replace('mp4','opus')\n",
    "        #print(vname,aname)\n",
    "\n",
    "        speaking_minutes = xls.parse(vname[:-4])\n",
    "        samples = ['Player'+str(e) for e in arange(1,11)]\n",
    "        interv = []\n",
    "        mafia_list = ['Player' + str(e) for e in gtruth[gtruth.File == vname][['M1','M2','M3']].values.ravel()]\n",
    "        #print(mafia_list)\n",
    "        for e in samples:\n",
    "            interv.append( [t2sec(e)  for e in speaking_minutes[e].values if str(e)!='nan'])\n",
    "        interv = array(interv)\n",
    "\n",
    "        audio_features = array(dataset[aname])\n",
    "        for idx,e in enumerate(samples):\n",
    "            video_features = array(dataset[vname][idx+1])\n",
    "            e1 = interv[idx]\n",
    "\n",
    "            for i in range(0,len(e1),2):\n",
    "                A = audio_features[e1[i]:e1[i+1]]\n",
    "                V = video_features[e1[i]:e1[i+1]]\n",
    "                for a,v in zip(A,V) :\n",
    "                    if dstype == 1:\n",
    "                        X.append(np.hstack([a]))\n",
    "                    elif dstype == 2:\n",
    "                        X.append(np.hstack([v]))\n",
    "                    elif dstype == 3:\n",
    "                        X.append(np.hstack([v,a]))\n",
    "                    if 'Player'+str(idx+1) in mafia_list:\n",
    "                        y.append(1)\n",
    "                    else:\n",
    "                        y.append(0)\n",
    "    X = array(X)\n",
    "    y = array(y)\n",
    "    return [X,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def werewolf_experiment(X, y, resample = 1):\n",
    "\n",
    "    params = {\n",
    "            'learning_rate': 0.1, \n",
    "            'n_estimators': 1000, \n",
    "            'seed': 0, \n",
    "            'subsample': 0.8, \n",
    "            'colsample_bytree': 0.8, \n",
    "            'objective': 'binary:logistic',\n",
    "    }\n",
    "    if resample == 2:\n",
    "        X = X.reshape([n//2,2*m])\n",
    "        y = array([(y[i] or y[i+1]) for i in range(0,len(y)-1,2)])\n",
    "\n",
    "    assert len(X) == len(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    gbt = XGBClassifier(**params)\n",
    "    gbt.fit(X_train,y_train)\n",
    "    y_pred = gbt.predict(X_test)\n",
    "    acc,f1 = accuracy_score(y_test,y_pred),f1_score(y_test,y_pred)\n",
    "    df = pd.DataFrame(confusion_matrix(y_test,y_pred))\n",
    "    return [acc,f1,df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X1,y1 = create_wlf_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2,y2 = create_wlf_dataset(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X3,y3 = create_wlf_dataset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res1 = werewolf_experiment(X=X1,y=y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res2 = werewolf_experiment(X=X2,y=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res3 = werewolf_experiment(X=X3,y=y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baseline = (accuracy_score(y1,zeros_like(y1)),\n",
    "            f1_score(y1,zeros_like(y1))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qtable = pd.DataFrame([baseline,res1[:2],res2[:2],res3[:2]],columns='Acc,F1'.split(','),\n",
    "                   index=['Baseline','Xgb+A','Xgb+V','Xgb+A+V']\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmatrix = pd.concat([res1[-1],res2[-1],res3[-1]])\n",
    "cmatrix.index = ('Mafia(WLF)','Citizen')*3\n",
    "cmatrix.columns = ('Mafia(WLF)','Citizen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmatrix"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
